{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/Images/'\n",
    "images = []\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            images.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import imgaug.augmenters as iaa\n",
    "import imgaug.augmentables.lines as ial\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, transform, aug=None, line_aug: bool = None):\n",
    "        self.images = images\n",
    "        self.cache = {}\n",
    "        self.transform = transform\n",
    "        self.aug = aug\n",
    "        self.line_aug = line_aug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_key = self.images[idx]\n",
    "        if image_key not in self.cache:\n",
    "            image = Image.open(image_key)\n",
    "            self.cache[image_key] = image\n",
    "        else:\n",
    "            image = self.cache[image_key]\n",
    "        image = self.transform(image)\n",
    "        train_image = image\n",
    "        W = image.shape[1]\n",
    "        if self.aug:\n",
    "            train_image = np.transpose(np.uint8(255 * image.numpy()), (1, 2, 0))\n",
    "            if self.line_aug:\n",
    "                line_aug = ial.LineStringsOnImage(\n",
    "                    [\n",
    "                        ial.LineString([(randrange(0, W), randrange(0, W)) for i in range(randrange(2, 5))]) for j\n",
    "                        in range(randrange(1, 3))\n",
    "                    ],\n",
    "                    shape=train_image.shape)\n",
    "                color_white = (randrange(235, 255), randrange(235, 255), randrange(235, 255))\n",
    "                size_white = randrange(2, 4)\n",
    "                train_image = line_aug.draw_on_image(train_image,\n",
    "                                                     color_lines=color_white, color_points=color_white,\n",
    "                                                     size_lines=size_white, size_points=size_white)\n",
    "            train_image = self.aug(image=train_image)\n",
    "            if self.line_aug:\n",
    "                line_aug = ial.LineStringsOnImage(\n",
    "                    [\n",
    "                        ial.LineString([(randrange(0, W), randrange(0, W)) for i in range(randrange(2, 5))]) for j\n",
    "                        in range(randrange(1, 5))\n",
    "                    ],\n",
    "                    shape=train_image.shape)\n",
    "                color_blue = (randrange(20, 80), randrange(20, 80), randrange(140, 220))\n",
    "                size_blue = randrange(1, 3)\n",
    "                train_image = line_aug.draw_on_image(train_image,\n",
    "                                                     color_lines=color_blue, color_points=color_blue,\n",
    "                                                     size_lines=size_blue, size_points=size_blue)\n",
    "            train_image = torch.from_numpy(np.transpose(train_image, (2, 0, 1)) / 255.0).float()\n",
    "        noise = torch.randn_like(train_image) * 0.05\n",
    "        noisy_image = train_image + noise\n",
    "        return noisy_image, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "valid_images, _ = train_test_split(images, test_size=0.98, random_state=42)\n",
    "train_images, test_images = train_test_split(valid_images, test_size=0.05, random_state=42)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomCrop(320),\n",
    "])\n",
    "aug = iaa.Sequential([\n",
    "    iaa.WithBrightnessChannels(iaa.Add((-60, 10))),\n",
    "])\n",
    "train_dataset = ImageDataset(train_images, transform=transform, aug=aug, line_aug=True)\n",
    "test_dataset = ImageDataset(test_images, transform=transform, aug=aug, line_aug=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarkMAELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DarkMAELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        # return torch.abs(inputs - targets).mean()\n",
    "        return ((1 / (1e-3 + torch.min(targets, 1 - targets))) * torch.abs(inputs - targets)).mean()\n",
    "        # return ((1 / (1e-4 + targets)) * torch.abs(inputs - targets)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.custom.net import ConvAutoencoder\n",
    "\n",
    "model = ConvAutoencoder()\n",
    "model = model.cuda()\n",
    "criterion = DarkMAELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5, threshold=0.001)\n",
    "print('Encoder Params:', sum(p.numel() for p in model.encoders.parameters()))\n",
    "print('Decoder Params:', sum(p.numel() for p in model.decoders.parameters()))\n",
    "# print('Total Params:', sum(p.numel() for p in model.parameters()))\n",
    "model.load_state_dict(torch.load(\"save/custom/model_1.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "losses = []\n",
    "pbar = tqdm(range(num_epochs))\n",
    "for epoch in pbar:\n",
    "    total_loss = 0\n",
    "    for iteration, data in enumerate(train_loader):\n",
    "        noisy_imgs, imgs = data\n",
    "        noisy_imgs = noisy_imgs.cuda()\n",
    "        imgs = imgs.cuda()\n",
    "\n",
    "        output = model(noisy_imgs)\n",
    "        loss = criterion(output, imgs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description('ITER: [{}/{}] | LOSS: {:.4f} | LR: {:.5f}'\n",
    "                             .format(iteration + 1, len(train_loader), total_loss / (iteration + 1),\n",
    "                                     optimizer.param_groups[0][\"lr\"]))\n",
    "    scheduler.step(total_loss / len(train_loader))\n",
    "    losses.append(total_loss / len(train_loader))\n",
    "    torch.save(model.state_dict(), \"save/custom/model_2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(test_loader, total=len(test_loader))\n",
    "    for data in pbar:\n",
    "        noisy_imgs, imgs = data\n",
    "        noisy_imgs = noisy_imgs.cuda()\n",
    "        imgs = imgs.cuda()\n",
    "        output = model(noisy_imgs)\n",
    "        loss = criterion(output, imgs)\n",
    "    pbar.set_description('test loss:{:.4f}'.format(loss.item()))\n",
    "\n",
    "noisy_imgs = noisy_imgs.cpu()\n",
    "imgs = imgs.cpu()\n",
    "output = output.cpu()\n",
    "fig, axes = plt.subplots(nrows=3, ncols=5, sharex=True, sharey=True, figsize=(25, 15))\n",
    "in_imgs = noisy_imgs[:5]\n",
    "reconstructed_imgs = output[:5]\n",
    "for images, row in zip([in_imgs, reconstructed_imgs, imgs], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(np.transpose(img, (1, 2, 0)))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "fig.tight_layout(pad=0.1)\n",
    "fig.savefig(\"out/custom/test_out.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    imgs = Image.open('D:/test_noisy.jpg')\n",
    "    imgs = imgs.crop((0, 0, 2544, 3504))\n",
    "    imgs = torch.unsqueeze(transforms.ToTensor()(imgs), dim=0)\n",
    "    imgs = imgs.cuda()\n",
    "    output = model(imgs)\n",
    "\n",
    "imgs = np.transpose(imgs.cpu().squeeze().numpy(), (1, 2, 0))\n",
    "output = np.transpose(output.cpu().squeeze().numpy(), (1, 2, 0))\n",
    "im = Image.fromarray(np.uint8(output * 255), mode='RGB')\n",
    "im.save(\"out/vanilla/test_noisy_out.jpg\")\n",
    "im = Image.fromarray(np.uint8(imgs * 255), mode='RGB')\n",
    "im.save(\"test_noisy.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
